export const projects = [

]

export const hard_skills = [
  "<b>Deep Learning</b> with PyTorch and TensorFlow",
  "<b>Computer Vision</b> with OpenCV and YOLO",
  "<b>Robotics Simulation</b> with MuJoCo, RoboSuite, and RoboMimic",
  "<b>Vehicle Simulation</b> with CARLA, OPV2V, and DAIR-V2X",
  "<b>Data Analysis</b> with Pandas, NumPy, and SciPy",
  "<b>CNNs</b>, <b>RNNs</b>, and <b>Transformers</b>",
];

export const soft_skills = [
  "<b>Literature Review</b> and Research Synthesis",
  "<b>Technical Writing</b> and Documentation",
  "<b>Publishing Research</b> Papers and Posters",
  "<b>Public Speaking</b> and Presentations",
  "<b>Project Management</b> and Organization",
  "<b>Academic Leadership</b> and Mentoring",
]

export const publications = [
  {
    title: "From Chaos to Clarity: Strengthening 3D Collaborative Autonomous Vehicle Perception with Noise-Aware Training",
    authors: ["Everett Richards", "Allie Lopez", "Jose Morales", "Ziming Zhang"],
    venue: "MIT Undergraduate Research Technology Conference",
    venueType: "Cambridge, MA",
    year: 2025,
    abstract: "Reliable perception is critical for autonomous vehicles (AVs), particularly in collaborative systems where multiple agents share sensor data to improve environmental awareness. However, most collaborative object detection frameworks assume ideal sensor conditions and degrade sharply when exposed to noise, occlusion, or hardware variabilityâ€”issues common in real-world deployment. In this work, we present a noise-aware training framework that improves the robustness and efficiency of 3D object detectors by injecting Gaussian noise into LiDAR point clouds during training. Using the BM2CP architecture and the DAIR-V2X dataset, we evaluated a range of training regimes, both constant and progressive, in more than 1,400 inference trials. We find that models trained with curriculum-style exposure to increasing noise levels degrade more gracefully under inference-time corruption, generalize better across sensor quality, and converge faster than traditional baselines. For instance, a model trained with heavy noise for just 20 epochs outperforms a baseline trained for 50 epochs when evaluated on degraded input, highlighting the accelerated rate of convergence enabled by noise-infused training. These findings demonstrate that robustness is a tunable and scalable property, offering a practical path toward safer and more cost-effective AV perception in noisy, uncertain, or resource-constrained environments.",
    thumbnail: "./research_videos/carla.gif",
    pdfLink: "./papers/Richards_BM2CP.pdf",
    officialLink: null,
    posterLink: "./posters/WPI.jpg",
    citations: {
      bibtex: `@inproceedings{richards2025chaos,
  title={From Chaos to Clarity: Strengthening 3D Collaborative Autonomous Vehicle Perception with Noise-Aware Training},
  author={Richards, Everett and Lopez, Allie and Morales, Jose and Zhang, Ziming},
  booktitle={MIT Undergraduate Research Technology Conference 2025},
  year={2025}
}`,
      apa: "Richards, E., Lopez, A., Morales, J., & Zhang, Z. (2025). From Chaos to Clarity: Strengthening 3D Collaborative Autonomous Vehicle Perception with Noise-Aware Training. In MIT Undergraduate Research Technology Conference 2025.",
      mla: 'Richards, Everett, et al. "From Chaos to Clarity: Strengthening 3D Collaborative Autonomous Vehicle Perception with Noise-Aware Training." MIT Undergraduate Research Technology Conference 2025. 2025.',
      chicago: "Richards, Everett, Allie Lopez, Jose Morales, and Ziming Zhang. \"From Chaos to Clarity: Strengthening 3D Collaborative Autonomous Vehicle Perception with Noise-Aware Training.\" In MIT Undergraduate Research Technology Conference 2025, 2025."
    }
  },
  {
    title: "Modeling Imitation Learning Robustness to Noisy Demonstrations via Sigmoid Degradation",
    authors: ["Everett Richards", "Liu Dai"],
    venue: "MIT Undergraduate Research Technology Conference",
    venueType: "Cambridge, MA",
    year: 2025,
    abstract: "Behavioral Cloning (BC) is a simple and widely used approach in Imitation Learning (IL), but its performance is highly sensitive to the quality of demonstration data. In this work, we evaluate the robustness of BC policies trained on noisy demonstrations generated by MimicGen, a synthetic data generation system that augments a small number of human demonstrations through scene variation and trajectory stitching. We systematically inject Gaussian spatial noise into the action trajectories of MimicGen-generated data and train BC policies across 11 robotic manipulation tasks. We find that small amounts of noise improve generalization, whereas higher levels predictably degrade policy success. To characterize this behavior, we introduce a four-parameter sigmoid model that captures the relationship between noise amplitude and downstream policy performance. Our model achieves an aggregate $R^2$ of 0.9962, and can be estimated using just one or two data points with under 4\% error. This framework offers a lightweight, quantitative tool for assessing demonstration quality and robustness in IL pipelines, supporting safer deployment of automated systems in domains where clean supervision data may be limited.",
    thumbnail: "./research_videos/mimicgen_coffee.gif",
    pdfLink: "./papers/Richards_MimicGen.pdf",
    officialLink: null,
    posterLink: null,
    citations: {
      bibtex: `@inproceedings{richards2025modeling,
  title={Modeling Imitation Learning Robustness to Noisy Demonstrations via Sigmoid Degradation},
  author={Richards, Everett and Dai, Liu},
  booktitle={MIT Undergraduate Research Technology Conference 2025},
  year={2025}
}`,
      apa: "Richards, E., & Dai, L. (2025). Modeling Imitation Learning Robustness to Noisy Demonstrations via Sigmoid Degradation. In MIT Undergraduate Research Technology Conference 2025.",
      mla: 'Richards, Everett, and Liu Dai. "Modeling Imitation Learning Robustness to Noisy Demonstrations via Sigmoid Degradation." MIT Undergraduate Research Technology Conference 2025. 2025.',
      chicago: "Richards, Everett, and Liu Dai. \"Modeling Imitation Learning Robustness to Noisy Demonstrations via Sigmoid Degradation.\" In MIT Undergraduate Research Technology Conference 2025, 2025."
    }
  },
  {
    title: "Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception",
    authors: ["Everett Richards", "Bipul Thapa", "Lena Mashayekhy"],
    venue: "IEEE International Conference on Edge Computing (EDGE)",
    venueType: "Helsinki, Finland",
    year: 2025,
    abstract: "Accurate and reliable object detection is critical for ensuring the safety and efficiency of Connected Autonomous Vehicles (CAVs). Traditional on-board perception systems have limited accuracy due to occlusions and blind spots, while cloud-based solutions introduce significant latency, making them unsuitable for real-time processing demands required for autonomous driving in dynamic environments. To address these challenges, we introduce an innovative framework, Edge-Enabled Collaborative Object Detection (ECOD) for CAVs, that leverages edge computing and multi-CAV collaboration for real-time, multi-perspective object detection. Our ECOD framework integrates two key algorithms: Perceptive Aggregation and Collaborative Estimation (PACE) and Variable Object Tally and Evaluation (VOTE). PACE aggregates detection data from multiple CAVs on an edge server to enhance perception in scenarios where individual CAVs have limited visibility. VOTE utilizes a consensus-based voting mechanism to improve the accuracy of object classification by integrating data from multiple CAVs. Both algorithms are designed at the edge to operate in real-time, ensuring low-latency and reliable decision-making for CAVs. We develop a hardware-based controlled testbed consisting of camera-equipped robotic CAVs and an edge server to evaluate the efficacy of our framework. Our experimental results demonstrate the significant benefits of ECOD in terms of improved object classification accuracy, outperforming traditional single-perspective onboard approaches by up to 75%, while ensuring low-latency, edge-driven real-time processing. This research highlights the potential of edge computing to enhance collaborative perception for latency-sensitive autonomous systems.",
    thumbnail: "./research_videos/reu_video.gif",
    pdfLink: "./papers/Richards_ECOD.pdf",
    officialLink: "https://ieeexplore.ieee.org/document/11120480",
    posterLink: "./posters/UDEL.jpg",
    citations: {
      bibtex: `@INPROCEEDINGS{11120480,
  author={Richards, Everett and Thapa, Bipul and Mashayekhy, Lena},
  booktitle={2025 IEEE International Conference on Edge Computing and Communications (EDGE)}, 
  title={Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception}, 
  year={2025},
  volume={},
  number={},
  pages={13-22},
  keywords={Accuracy;Image edge detection;Collaboration;Object detection;Real-time systems;Classification algorithms;Servers;Vehicle dynamics;Autonomous vehicles;Edge computing},
  doi={10.1109/EDGE67623.2025.00011}}
}`,
      apa: "Richards, E., Thapa, B., & Mashayekhy, L. (2025). Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception. In 2025 IEEE International Conference on Edge Computing and Scalable Cloud (EDGE) (pp. 1-8). IEEE.",
      mla: 'Richards, Everett, et al. "Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception." 2025 IEEE International Conference on Edge Computing and Scalable Cloud (EDGE). IEEE, 2025.',
      chicago: "Richards, Everett, Bipul Thapa, and Lena Mashayekhy. \"Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception.\" In 2025 IEEE International Conference on Edge Computing and Scalable Cloud (EDGE), pp. 1-8. IEEE, 2025."
    }
  },
];

export const researches = [
  {
    title: "From Chaos to Clarity: Strengthening 3D Collaborative Autonomous Vehicle Perception with Noise-Aware Training",
    lab: "NSF REU in Applied Artificial Intelligence for Advanced Applications",
    university: "Worcester Polytechnic Institute (WPI)",
    when: "Fall 2024 -- Spring 2025",
    description:
      "Developed and analyzed a multimodal collaborative object detection algorithm integrating LiDAR-camera sensor fusion to enhance 3D perception in autonomous vehicles. Conducted robustness evaluations through targeted injection of Gaussian noise into 3D LiDAR point clouds, simulating sensor degradation due to low-resolution sensors and adverse weather conditions. Proposed and empirically validated a novel noise-aware training curriculum, achieving up to a 40% improvement in model robustness under challenging real-world scenarios. Resulting paper accepted at MIT URTC 2025.",
    image: "./research_videos/carla.gif",
    links: {
      "Paper": "./papers/Richards_BM2CP.pdf",
      "Poster": "./posters/WPI.jpg"
    },
    //main_link: "./papers/Richards_BM2CP.pdf",
  },
  {
    title: "Modeling Imitation Learning Robustness to Noisy Demonstrations via Sigmoid Degradation",
    lab: "NSF REU in Interdisciplinary Artificial Intelligence",
    university: "University of California San Diego (UCSD)",
    when: "Fall 2024 -- Spring 2025",
    description:
      "Conducted experiments on the impact of Gaussian noise injection in robotic imitation learning models. Optimized a sigmoid approximation curve relating noise amplitude to performance, yielding statistically significant R^2 values between 0.91 and 0.99. Used Pandas, NumPy, MatPlotLib, and SciPy for data analysis and optimization. Used MuJoCo, RoboSuite, RoboMimic, and MimicGen for robotic simulation and imitation learning. Resulting paper accepted at MIT URTC 2025.",
    image: "./research_videos/mimicgen_coffee.gif",
    links: {
      "Paper": "./papers/Richards_MimicGen.pdf",
    },
    //main_link: "https://mimicgen.github.io/",
  },
  {
    title: "Edge-Enabled Collaborative Object Detection for Real-Time Multi-Vehicle Perception",
    lab: "NSF REU in Sustainable Resilient Transportation Systems",
    university: "University of Delaware (UD)",
    when: "Summer 2024",
    description:
      "At the NSF REU, I developed and tested an interdisciplinary framework to improve autonomous vehicle object labeling accuracy by combining multiple vehicles' perspectives via an edge server. I accomplished this by developing two algorithms, called Perceptive Aggregation and Collaborative Estimation (PACE) and Variable Object Tally and Evaluation (VOTE). I presented a poster at the University of Delaware Undergraduate Research Symposium in August 2024, and presented my work at the IEEE EDGE conference in Helsinki, Finland in July 2025.",
    image: "./research_videos/reu_video.gif",
    links: {
      "IEEE Xplore": "https://ieeexplore.ieee.org/document/11120480",
      "Paper": "./papers/Richards_ECOD.pdf",
      "Poster": "./posters/UDEL.jpg",
      "Code": "https://github.com/EverettRichards/Edge-CAV",
    },
    //main_link: "./papers/Richards_ECOD.pdf",
  },
  {
    title: "Enhancing Construction Worker Safety Through Sensor Fusion and Machine Learning",
    lab: "Data-informed Construction Engineering (DiCE) Laboratory",
    university: "San Diego State University (SDSU)",
    when: "Spring 2024",
    description:
      "At the DiCE Lab, I designed a data analytics program that fuses IMU sensors and camera input to classify and predict the movements of construction workers. I also developed a 3D simulation to visualize IMU data in real-time, allowing for the data to be validated.",
    image: "./research_videos/dice_video.gif",
    links: {
      "Lab": "https://dice.sdsu.edu/",
      "Code": "https://github.com/EverettRichards/DiCE",
      "Demo": "https://www.youtube.com/watch?v=3qlrGciOR84",
    },
    //main_link: "https://github.com/EverettRichards/DiCE",
  },
];

export const contacts = [
  {
    title: "Email",
    value: "ehrichards9@gmail.com",
    url: "mailto:ehrichards9@gmail.com",
    icon: "./icons/gmail.svg",
  },
  {
    title: "LinkedIn",
    value: "linkedin.com/in/everett-richards",
    url: "https://www.linkedin.com/in/everett-richards",
    icon: "./icons/linkedin.svg",
  },
  {
    title: "GitHub",
    value: "github.com/EverettRichards",
    url: "https://github.com/EverettRichards",
    icon: "./icons/github.svg",
  },
  {
    title: "Instagram",
    value: "@the_everett_richards",
    url: "https://www.instagram.com/the_everett_richards/",
    icon: "./icons/instagram.svg",
  },
  {
    title: "Google Scholar",
    value: "Google Scholar",
    url: "https://scholar.google.com/citations?user=iowXMSwAAAAJ&hl=en",
    icon: "./icons/google-scholar.svg",
  },
  {
    title: "ORCID",
    value: "ORCID",
    url: "https://orcid.org/0009-0009-9743-5636",
    icon: "./icons/orcid.svg",
  }
]

export const slides = [
  {"photo": "./slideshow/acm_meeting.jpg",
    "caption": "President of the ACM Chapter at SDSU",},
  {"photo": "./slideshow/Teddie2.jpg",
    "caption": "Exploring the nation's capital with a family member's dog, Teddie.",},
  {"photo": "./slideshow/ud_symposium.jpg",
    "caption": "Presenting my research at the University of Delaware Undergraduate Research Symposium in August 2024.",},
  {"photo": "./slideshow/kayaking_providence.jpg",
    "caption": "Kayaking on the Providence River in Rhode Island.",},
  {"photo": "./slideshow/acm_lecture.jpg",
    "caption": "Presenting a workshop on machine learning at the SDSU ACM Chapter.",},
    {"photo": "./slideshow/tutoring.jpg",
    "caption": "Tutoring Discrete Mathematics at the Math & Science Learning Center (MSLC) at San Diego State University.",},
  {"photo": "./slideshow/EstesPark.jpg",
    "caption": "Adventuring at Estes Park in the Colorado Rocky Mountains.",},
  {"photo": "./slideshow/NICU.jpg",
    "caption": "Celebrating my receipt of the North Island Credit Union scholarship.",},
  // {"photo": "./slideshow/WyomingCapitol.jpg",
  //   "caption": "Touring the Wyoming State Capitol in Cheyenne.",},
  {"photo": "./slideshow/AtWorkREU.jpg",
    "caption": "Working on my research project for the NSF REU at the University of Delaware.",},
];
